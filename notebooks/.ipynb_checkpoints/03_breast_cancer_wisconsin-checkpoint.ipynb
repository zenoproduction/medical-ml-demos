{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Breast Cancer Wisconsin \u2013 Classificazione\n**Pipeline:** dataset \u2192 preprocessing \u2192 training \u2192 valutazione (ROC/AUC, conf. matrix)\nDataset disponibile in `scikit-learn`."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# %pip install scikit-learn matplotlib seaborn"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nSEED = 42\ndata = load_breast_cancer(as_frame=True)\nX, y = data.data, data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)"}, {"cell_type": "markdown", "metadata": {}, "source": "## 1) Confronto modelli con pipeline e CV"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "models = {\n    'LogReg': Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression(max_iter=1000, random_state=SEED))]),\n    'RF': RandomForestClassifier(n_estimators=300, max_depth=None, random_state=SEED),\n    'SVM(RBF)': Pipeline([('scaler', StandardScaler()), ('clf', SVC(probability=True, kernel='rbf', C=2.0, gamma='scale', random_state=SEED))]),\n}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\nfor name, model in models.items():\n    auc = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc').mean()\n    print(f'CV AUC \u2013 {name}: {auc:.3f}')\n\nfitted = {name: model.fit(X_train, y_train) for name, model in models.items()}"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2) ROC multi-modello su test"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "plt.figure()\nfor name, model in fitted.items():\n    y_prob = model.predict_proba(X_test)[:,1]\n    auc = roc_auc_score(y_test, y_prob)\n    fpr, tpr, _ = roc_curve(y_test, y_prob)\n    plt.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})')\nplt.plot([0,1],[0,1],'--')\nplt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC \u2013 Breast Cancer Wisconsin'); plt.legend(); plt.show()\n\n# Confusion matrix per il migliore (scelgo quello con AUC pi\u00f9 alto sul test)\nbest_name, best_model, best_auc = None, None, -1\nfor name, model in fitted.items():\n    y_prob = model.predict_proba(X_test)[:,1]\n    auc = roc_auc_score(y_test, y_prob)\n    if auc > best_auc: best_auc, best_name, best_model = auc, name, model\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\ny_pred = best_model.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(f'Miglior modello su test: {best_name} (AUC={best_auc:.3f})')\nprint('Confusion Matrix:\\n', cm)"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}